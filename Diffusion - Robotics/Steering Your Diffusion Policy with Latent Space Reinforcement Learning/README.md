https://arxiv.org/abs/2506.15799

**Relatedness score: 5 / 10**

| Why it helps (adds +)                                                                                                                                                                                                                                                                                     | Why it doesn’t (+ –)                                                                                                                                                                                            |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Steers a diffusion/FM model *without weight updates*.** The paper’s “diffusion steering via RL” learns a tiny latent-noise policy to bias a frozen generator — exactly the kind of *API-side* guidance knob your SaaS could expose so customers refine candidates without retraining full checkpoints.  | **Single-objective, robotics-only.** Reward is a scalar task return in MuJoCo or real robots; no weight-vector conditioning or Pareto-front reasoning, so it doesn’t solve the multi-objective protein problem. |
| **Flow-matching variant exists.** Authors note that the same trick works for flow-matching policies, so the mechanics are transferable to your discrete FM backbone.                                                                                                                                      | **Continuous actions vs. amino-acid tokens.** The policy outputs real-valued torques, not categorical residues; you would have to port the method to CTMC/discrete heads.                                       |
| **Black-box friendly.** Steering requires only forward passes of the base model—aligns with your vision of letting clients bring proprietary surrogates or generators behind a VPC and still use your Pareto layer.                                                                                       | **No wet-lab cost motivation.** Their “expensive” loop is GPU time, not \$250–\$1 000 assays, so the business pain and willingness-to-pay story differs.                                                        |
| **Sample efficiency proof.** They show 5-10 × fewer environment interactions than other RL finetunes—evidence that noise-space control can cut evaluation calls, a theme your platform monetises.                                                                                                         | **Compliance & domain gaps.** Paper ignores GxP/HIPAA, toxicity, immunogenicity—core selling points for a protein/peptide SaaS.                                                                                 |

**Take-away:**
The paper is a useful *engineering inspiration* for “steer-at-inference-time” control of FM models, but because it is single-objective, continuous-action, and robotics-centric, it only partially overlaps with your Pareto, discrete-token, wet-lab-cost value proposition—landing at a mid-level **5 / 10** relevance.
